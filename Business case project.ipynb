{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "883d2e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9d7b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_csv_data = np.loadtxt(\"Audiobooks_data.csv\",delimiter=',') \n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
    "targets_all =raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c7cc0e",
   "metadata": {},
   "source": [
    "## Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5db25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_targets = int(np.sum(targets_all))\n",
    "zero_targets_counter = 0\n",
    "indices_to_remove = []\n",
    "\n",
    "\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i]==0:\n",
    "        zero_targets_counter+=1\n",
    "        if zero_targets_counter>num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "            \n",
    "unscaled_inputs_with_equal_priors = np.delete(unscaled_inputs_all,indices_to_remove,axis=0)\n",
    "targets_equal_priors = np.delete(targets_all,indices_to_remove,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f892c97",
   "metadata": {},
   "source": [
    "## Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25bfcb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_inputs_with_equal_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ab3eb",
   "metadata": {},
   "source": [
    "## Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56b0b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360cc9b0",
   "metadata": {},
   "source": [
    "## Split the data into train, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7b8a38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1783.0 3579 0.4981838502374965\n",
      "234.0 447 0.5234899328859061\n",
      "220.0 447 0.49217002237136465\n"
     ]
    }
   ],
   "source": [
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "train_samples_count = int(0.8*samples_count)\n",
    "validation_samples_count = int(0.1*samples_count)\n",
    "test_samples_count = int(0.1*samples_count)\n",
    "\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
    "\n",
    "print(np.sum(train_targets),train_samples_count,np.sum(train_targets)/train_samples_count)\n",
    "print(np.sum(validation_targets),validation_samples_count,np.sum(validation_targets)/validation_samples_count)\n",
    "print(np.sum(test_targets),test_samples_count,np.sum(test_targets)/test_samples_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2474822e",
   "metadata": {},
   "source": [
    "## Save the datasets in *.npz format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96435749",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"Audiobooks_data_train\",inputs=train_inputs,targets=train_targets)\n",
    "np.savez(\"Audiobooks_data_validation\",inputs=validation_inputs,targets=validation_targets)\n",
    "np.savez(\"Audiobooks_data_test\",inputs=test_inputs,targets=test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350ef669",
   "metadata": {},
   "source": [
    "## Creating ML algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9fbe0693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1677e87",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48ef550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "npz1 = np.load(\"Audiobooks_data_train.npz\")\n",
    "\n",
    "train_inputs = npz1['inputs'].astype(float)\n",
    "train_targets = npz1['targets'].astype(int)\n",
    "\n",
    "npz2 = np.load(\"Audiobooks_data_validation.npz\")\n",
    "validation_inputs, validation_targets =  npz2['inputs'].astype(float),npz2['targets'].astype(int)\n",
    "\n",
    "npz3 = np.load(\"Audiobooks_data_test.npz\")\n",
    "test_inputs,test_targets = npz3[\"inputs\"].astype(float),npz3[\"targets\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cedd4a2",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a7482b",
   "metadata": {},
   "source": [
    "## Outline,loss, early stopping and training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d31665e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "output_size = 2\n",
    "hidden_layer_size = 100\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            \n",
    "                            tf.keras.layers.Dense(hidden_layer_size,activation = \"relu\"),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size,activation = \"relu\"),\n",
    "                            tf.keras.layers.Dense(output_size,activation = \"softmax\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b920be",
   "metadata": {},
   "source": [
    "## Choose optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa172219",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer =\"adam\", loss = \"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])  \n",
    "##sparse_categorical_crossentropy is used to ensure that the targets are one-hot encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a18b5e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "72/72 - 1s - loss: 0.4820 - accuracy: 0.7499 - val_loss: 0.4305 - val_accuracy: 0.7606 - 912ms/epoch - 13ms/step\n",
      "Epoch 2/10000\n",
      "72/72 - 0s - loss: 0.3859 - accuracy: 0.7899 - val_loss: 0.3855 - val_accuracy: 0.8009 - 143ms/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "72/72 - 0s - loss: 0.3597 - accuracy: 0.8089 - val_loss: 0.3699 - val_accuracy: 0.7987 - 158ms/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "72/72 - 0s - loss: 0.3510 - accuracy: 0.8055 - val_loss: 0.3595 - val_accuracy: 0.8009 - 149ms/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "72/72 - 0s - loss: 0.3476 - accuracy: 0.8128 - val_loss: 0.3632 - val_accuracy: 0.8143 - 149ms/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "72/72 - 0s - loss: 0.3364 - accuracy: 0.8150 - val_loss: 0.3563 - val_accuracy: 0.8031 - 148ms/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "72/72 - 0s - loss: 0.3346 - accuracy: 0.8120 - val_loss: 0.3516 - val_accuracy: 0.8277 - 149ms/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "72/72 - 0s - loss: 0.3360 - accuracy: 0.8122 - val_loss: 0.3477 - val_accuracy: 0.8166 - 148ms/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "72/72 - 0s - loss: 0.3337 - accuracy: 0.8134 - val_loss: 0.3579 - val_accuracy: 0.7942 - 151ms/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "72/72 - 0s - loss: 0.3315 - accuracy: 0.8161 - val_loss: 0.3444 - val_accuracy: 0.8143 - 148ms/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "72/72 - 0s - loss: 0.3280 - accuracy: 0.8229 - val_loss: 0.3657 - val_accuracy: 0.8166 - 159ms/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "72/72 - 0s - loss: 0.3309 - accuracy: 0.8142 - val_loss: 0.3421 - val_accuracy: 0.8188 - 157ms/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "72/72 - 0s - loss: 0.3195 - accuracy: 0.8251 - val_loss: 0.3408 - val_accuracy: 0.8322 - 150ms/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "72/72 - 0s - loss: 0.3270 - accuracy: 0.8198 - val_loss: 0.3407 - val_accuracy: 0.8233 - 150ms/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "72/72 - 0s - loss: 0.3274 - accuracy: 0.8148 - val_loss: 0.3525 - val_accuracy: 0.8031 - 150ms/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "72/72 - 0s - loss: 0.3233 - accuracy: 0.8164 - val_loss: 0.3495 - val_accuracy: 0.8345 - 150ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27e0c62deb0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "max_epochs = 10000\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "model.fit(train_inputs,train_targets,\n",
    "          batch_size=batch_size,epochs=max_epochs,callbacks=[early_stopping],\n",
    "          validation_data=(validation_inputs,validation_targets),\n",
    "         verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025f9a4f",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c968b33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3412 - accuracy: 0.8326\n"
     ]
    }
   ],
   "source": [
    "test_loss,test_accuracy = model.evaluate(test_inputs,test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c409c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c19ff97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64385079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e946efd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
